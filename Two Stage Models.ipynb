{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Purpose: Two stage model. We use multiple classifiers at the base-stage to generate additional (prediction) features. \n",
    "A single meta-classifier is then used to make a final prediction\n",
    "\n",
    "See: https://github.com/daxiongshu/kaggle-tradeshift-winning-solution/blob/master/TradeshiftTextClassification.pdf\n",
    "'''\n",
    "\n",
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "import pandas as pd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n"
     ]
    }
   ],
   "source": [
    "### Load some data\n",
    "from sklearn.datasets import load_digits \n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Just break it up to some train/test set\n",
    "X, y = digits['data'], digits['target']\n",
    "X, X_sumbission = X[0:1300],X[1300:]\n",
    "y, y_submission_actual = y[0:1300],y[1300:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1300, 64)\n",
      "(497, 64)\n",
      "(1300,)\n"
     ]
    }
   ],
   "source": [
    "print np.shape(X)\n",
    "print np.shape(X_submission)\n",
    "print np.shape(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(650, 64)\n",
      "(650, 64)\n",
      "(650,)\n",
      "(650,)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Split training set into 'base' and 'meta' components.\n",
    "This can be done 3 ways:\n",
    "    1) Random 50-50 Split\n",
    "    2) First half for base, second half for meta\n",
    "    3) First half for meta, second half for base\n",
    "    \n",
    "You should test all three ways and pick which gives best scores \n",
    "    -OR- just do all three and ensemble them together at the end\n",
    "'''\n",
    "\n",
    "## For now, just split random 50%\n",
    "X_base, X_meta, y_base, y_meta = train_test_split(X, y, test_size=0.5, random_state=42)\n",
    "\n",
    "print np.shape(X_base)\n",
    "print np.shape(X_meta)\n",
    "print np.shape(y_base)\n",
    "print np.shape(y_meta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training....\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train on base data and predict on meta\n",
    "\n",
    "Here we can train mutiple models, all parameters should be tuned using a grid search and CV\n",
    "'''\n",
    "\n",
    "### We can use as many classifiers as we want \n",
    "clf_base_rf = RandomForestClassifier(n_estimators=10, n_jobs=-1, criterion='gini')\n",
    "clf_base_et = ExtraTreesClassifier(n_estimators=10, n_jobs=-1, criterion='gini')\n",
    "       \n",
    "### We only train on the 'base' part of the data\n",
    "clf_base_rf.fit(X_base, y_base)\n",
    "clf_base_et.fit(X_base, y_base)\n",
    "\n",
    "print \"Done training....\"\n",
    "\n",
    "X_meta_pred_rf = clf_base_rf.predict_proba(X_meta)[:,1]\n",
    "X_meta_pred_et = clf_base_et.predict_proba(X_meta)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training....\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Train meta data with raw features and meta features (predictions).\n",
    "This only uses ONE classifier and combines all the raw + meta features\n",
    "'''\n",
    "\n",
    "### Here we build only a single classfier \n",
    "clf_meta = RandomForestClassifier(n_estimators=100, n_jobs=-1, criterion='gini')\n",
    "\n",
    "### The meta classifier is trained with all raw and prediction features\n",
    "clf_meta.fit( np.column_stack((X_meta, X_meta_pred_rf, X_meta_pred_et)), y_meta) \n",
    "\n",
    "print \"Done training....\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done training....\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "For the test set, we want to train on ALL the avaliable data!\n",
    "In this way, the meta features of the test set become more informative, \n",
    "hence generating more accurate final predictions.\n",
    "'''\n",
    "\n",
    "### We only train on the full the data\n",
    "clf_base_rf.fit(X, y)\n",
    "clf_base_et.fit(X, y)\n",
    "\n",
    "print \"Done training....\"\n",
    "\n",
    "X_test_pred_rf = clf_base_rf.predict_proba(X_sumbission)[:,1]\n",
    "X_test_pred_et = clf_base_et.predict_proba(X_sumbission)[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Final prediction using the meta model\n",
    "'''\n",
    "\n",
    "pred = clf_meta.predict_proba( np.column_stack((X_submission, X_test_pred_rf, X_test_pred_et)) )[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#'''\n",
    "#We can also use training on the full set of data with other classifiers\n",
    "#'''\n",
    "#clf_full_lr = LogisticRegression()\n",
    "#clf_full_lr.fit(X,y)\n",
    "#X_test_pred_full_lf = clf_full_lr.predict_proba(X_sumbission)[:,1]\n",
    "#\n",
    "#pred2 = clf_meta.predict_proba( np.column_stack((X_submission, X_test_pred_rf, X_test_pred_et, X_test_pred_full_lf)) )[:,1]\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
